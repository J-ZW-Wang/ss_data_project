{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Detection on Websites\n",
    "Source: https://platform.stratascratch.com/data-projects/keyword-detection-websites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Load and examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindt = pd.read_csv(\"./datasets/train.csv\")\n",
    "testdt = pd.read_csv(\"./datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://chirurgie-goettingen.de/medizinische-ve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://evkb.de/kliniken-zentren/chirurgie/allg...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://krebszentrum.kreiskliniken-reutlingen.d...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://marienhospital-buer.de/mhb-av-chirurgie...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://marienhospital-buer.de/mhb-av-chirurgie...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  doc_id\n",
       "0  http://chirurgie-goettingen.de/medizinische-ve...       0\n",
       "1  http://evkb.de/kliniken-zentren/chirurgie/allg...       2\n",
       "2  http://krebszentrum.kreiskliniken-reutlingen.d...       7\n",
       "3  http://marienhospital-buer.de/mhb-av-chirurgie...      15\n",
       "4  http://marienhospital-buer.de/mhb-av-chirurgie...      16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jerry_Dell_7060\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "htmldt_0 = []\n",
    "\n",
    "for i in range(147):\n",
    "    with open(f\"./datasets/htmls/{i}.html\", \"rb\") as f:\n",
    "        initread = chardet.detect(f.read())\n",
    "        encoding = initread[\"encoding\"]\n",
    "\n",
    "    with open(f\"./datasets/htmls/{i}.html\", \"r\", encoding=encoding) as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    body = soup.get_text(\" \", strip=True)\n",
    "    htmldt_0.append({\"doc_id\": i, \"html\": body})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmldt = (\n",
    "    pd.DataFrame(htmldt_0)\n",
    "    .merge(traindt, on=\"doc_id\", how=\"left\", validate=\"1:1\")\n",
    "    .assign(label_pred=None, label=lambda x: x.label.fillna(0).astype(int) - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>html</th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>label_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bauchspeicheldrüse | Klinik für Allgemein-, Vi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Elbe-Elster Klinikum - Chirurgie Finsterwalde ...</td>\n",
       "      <td>http://elbe-elster-klinikum.de/fachbereiche/ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Chirurgie der Bauchspeicheldrüse (Pankreaschir...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Onkologisches Zentrum - Klinikum Bayreuth Aktu...</td>\n",
       "      <td>http://klinikum-bayreuth.de/einrichtungen/zent...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Zentrum - Sozialpädiatrisches Zentrum - Städti...</td>\n",
       "      <td>http://klinikum-braunschweig.de/info.php/?id_o...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>Tumorboard des Lungenkrebszentrums Möglicherwe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>143</td>\n",
       "      <td>Oberarzt (m/w/d) für die HNO-Klinik A- A A+ No...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>Für Ärzte | Vivantes JavaScript scheint in Ihr...</td>\n",
       "      <td>http://www.vivantes.de/fuer-sie-vor-ort/klinik...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>Innere Medizin – Hämatologie, Onkologie und Pa...</td>\n",
       "      <td>http://www.vivantes.de/fuer-sie-vor-ort/klinik...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>Versorgung von Krebspatienten stärken | St. Wa...</td>\n",
       "      <td>http://www.walburga-krankenhaus.de/wk/artikel/...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                               html  \\\n",
       "0         0  Bauchspeicheldrüse | Klinik für Allgemein-, Vi...   \n",
       "1         1  Elbe-Elster Klinikum - Chirurgie Finsterwalde ...   \n",
       "2         2  Chirurgie der Bauchspeicheldrüse (Pankreaschir...   \n",
       "3         3  Onkologisches Zentrum - Klinikum Bayreuth Aktu...   \n",
       "4         4  Zentrum - Sozialpädiatrisches Zentrum - Städti...   \n",
       "..      ...                                                ...   \n",
       "142     142  Tumorboard des Lungenkrebszentrums Möglicherwe...   \n",
       "143     143  Oberarzt (m/w/d) für die HNO-Klinik A- A A+ No...   \n",
       "144     144  Für Ärzte | Vivantes JavaScript scheint in Ihr...   \n",
       "145     145  Innere Medizin – Hämatologie, Onkologie und Pa...   \n",
       "146     146  Versorgung von Krebspatienten stärken | St. Wa...   \n",
       "\n",
       "                                                   url  label label_pred  \n",
       "0                                                  NaN     -1       None  \n",
       "1    http://elbe-elster-klinikum.de/fachbereiche/ch...      0       None  \n",
       "2                                                  NaN     -1       None  \n",
       "3    http://klinikum-bayreuth.de/einrichtungen/zent...      2       None  \n",
       "4    http://klinikum-braunschweig.de/info.php/?id_o...      0       None  \n",
       "..                                                 ...    ...        ...  \n",
       "142                                                NaN     -1       None  \n",
       "143                                                NaN     -1       None  \n",
       "144  http://www.vivantes.de/fuer-sie-vor-ort/klinik...      1       None  \n",
       "145  http://www.vivantes.de/fuer-sie-vor-ort/klinik...      1       None  \n",
       "146  http://www.walburga-krankenhaus.de/wk/artikel/...      0       None  \n",
       "\n",
       "[147 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmldt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      147.000000\n",
       "mean      1883.755102\n",
       "std       3867.911788\n",
       "min        206.000000\n",
       "25%        691.500000\n",
       "50%        966.000000\n",
       "75%       1832.500000\n",
       "max      36214.000000\n",
       "Name: html, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmldt.html.str.split().apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) LLM based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use the model\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b2d0d215b645d980843746907f9637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m      5\u001b[0m context \u001b[38;5;241m=\u001b[39m htmldt\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m QA_Input \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     {\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     },\n\u001b[0;32m     24\u001b[0m ]\n\u001b[1;32m---> 26\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m(\n\u001b[0;32m     27\u001b[0m     QA_Input, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens, pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128001\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m     30\u001b[0m     item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 10\n",
    "\n",
    "# for i in tqdm(range(htmldt.shape[0])):\n",
    "for i in tqdm(range(1)):  # for testing purposes\n",
    "    context = htmldt.iloc[i][\"html\"]\n",
    "\n",
    "    QA_Input = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            You will review text extracted from German HTML webpages and respond in English.\n",
    "            Tumor Board is a consilium of doctors (usually from different disciplines) discussing cancer cases in their departments.\n",
    "            Task: Determine if the text refers to tumor boards and assign one of the following labels:\n",
    "            - '1': No mention of tumor boards.\n",
    "            - '2': Mentions tumor boards, but the page is not fully dedicated to their description.\n",
    "            - '3': The page is fully dedicated to describing tumor board types and dates.\n",
    "\n",
    "            Respond strictly with one of the numbers ('1', '2', or '3'). Do not include any additional text. \n",
    "            Limit your response to {max_new_tokens} tokens.\n",
    "\n",
    "            Text for review: {context}\n",
    "            \"\"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    res = pipe(\n",
    "        QA_Input, max_new_tokens=max_new_tokens, pad_token_id=128001, temperature=0.001\n",
    "    )\n",
    "    label = next(\n",
    "        item[\"content\"]\n",
    "        for item in res[0][\"generated_text\"]\n",
    "        if item[\"role\"] == \"assistant\"\n",
    "    )\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "    htmldt.loc[i, \"label_pred\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_pred\n",
       "1. No Evidence                                        62\n",
       "1. No Evidence\\n2. Medium confidence\\n                31\n",
       "1                                                      9\n",
       "'3'                                                    7\n",
       "1. No                                                  7\n",
       "Tumor Board is a consilium of doctors                  5\n",
       "Here are the three labels for the text:\\n\\n1           4\n",
       "1.                                                     4\n",
       "3                                                      3\n",
       "1. '3'                                                 2\n",
       "The text is about the Sana Klinikum                    1\n",
       "This text appears to be a list of albums by            1\n",
       "No Evidence                                            1\n",
       "Here are the three labels for the given text:\\n\\n      1\n",
       "Tumor Board: '3'                                       1\n",
       "Tumor Board: '3' (High confidence                      1\n",
       "Here is the review of the text in English:\\n\\n         1\n",
       "Tumor Board ist eine Konsultation von Ä                1\n",
       "GPR Gesundheits- und Pflegezent                        1\n",
       "1. '3' (High confidence)                               1\n",
       "I cannot fulfill your request as it is against the     1\n",
       "The text is a German text about the Onk                1\n",
       "The text is a job posting for an Oberar                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmldt.label_pred.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results using LLM based prompt engineering were not optimal due to several reasons:\n",
    "1. Due to cost and hardware constrains, the most powerful models were not available for this exercise. \n",
    "2. The concept of 'Tumor board' may be unclear to the LLM. The training data was not fully utilized to influence the output.\n",
    "\n",
    "For future work, the OpenAI GPT models or more powerful open LLMs from huggingface platforms could be tested for enhanced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Supervised training with transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-base-german-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "hfmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31102, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(hfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze base model parameters\n",
    "for name, param in hfmodel.base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze base model pooling layers\n",
    "for name, param in hfmodel.base_model.named_parameters():\n",
    "    if \"pooler\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight False\n",
      "bert.embeddings.position_embeddings.weight False\n",
      "bert.embeddings.token_type_embeddings.weight False\n",
      "bert.embeddings.LayerNorm.weight False\n",
      "bert.embeddings.LayerNorm.bias False\n",
      "bert.encoder.layer.0.attention.self.query.weight False\n",
      "bert.encoder.layer.0.attention.self.query.bias False\n",
      "bert.encoder.layer.0.attention.self.key.weight False\n",
      "bert.encoder.layer.0.attention.self.key.bias False\n",
      "bert.encoder.layer.0.attention.self.value.weight False\n",
      "bert.encoder.layer.0.attention.self.value.bias False\n",
      "bert.encoder.layer.0.attention.output.dense.weight False\n",
      "bert.encoder.layer.0.attention.output.dense.bias False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.0.intermediate.dense.weight False\n",
      "bert.encoder.layer.0.intermediate.dense.bias False\n",
      "bert.encoder.layer.0.output.dense.weight False\n",
      "bert.encoder.layer.0.output.dense.bias False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.attention.self.query.weight False\n",
      "bert.encoder.layer.1.attention.self.query.bias False\n",
      "bert.encoder.layer.1.attention.self.key.weight False\n",
      "bert.encoder.layer.1.attention.self.key.bias False\n",
      "bert.encoder.layer.1.attention.self.value.weight False\n",
      "bert.encoder.layer.1.attention.self.value.bias False\n",
      "bert.encoder.layer.1.attention.output.dense.weight False\n",
      "bert.encoder.layer.1.attention.output.dense.bias False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.intermediate.dense.weight False\n",
      "bert.encoder.layer.1.intermediate.dense.bias False\n",
      "bert.encoder.layer.1.output.dense.weight False\n",
      "bert.encoder.layer.1.output.dense.bias False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.attention.self.query.weight False\n",
      "bert.encoder.layer.2.attention.self.query.bias False\n",
      "bert.encoder.layer.2.attention.self.key.weight False\n",
      "bert.encoder.layer.2.attention.self.key.bias False\n",
      "bert.encoder.layer.2.attention.self.value.weight False\n",
      "bert.encoder.layer.2.attention.self.value.bias False\n",
      "bert.encoder.layer.2.attention.output.dense.weight False\n",
      "bert.encoder.layer.2.attention.output.dense.bias False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.intermediate.dense.weight False\n",
      "bert.encoder.layer.2.intermediate.dense.bias False\n",
      "bert.encoder.layer.2.output.dense.weight False\n",
      "bert.encoder.layer.2.output.dense.bias False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.attention.self.query.weight False\n",
      "bert.encoder.layer.3.attention.self.query.bias False\n",
      "bert.encoder.layer.3.attention.self.key.weight False\n",
      "bert.encoder.layer.3.attention.self.key.bias False\n",
      "bert.encoder.layer.3.attention.self.value.weight False\n",
      "bert.encoder.layer.3.attention.self.value.bias False\n",
      "bert.encoder.layer.3.attention.output.dense.weight False\n",
      "bert.encoder.layer.3.attention.output.dense.bias False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.intermediate.dense.weight False\n",
      "bert.encoder.layer.3.intermediate.dense.bias False\n",
      "bert.encoder.layer.3.output.dense.weight False\n",
      "bert.encoder.layer.3.output.dense.bias False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.attention.self.query.weight False\n",
      "bert.encoder.layer.4.attention.self.query.bias False\n",
      "bert.encoder.layer.4.attention.self.key.weight False\n",
      "bert.encoder.layer.4.attention.self.key.bias False\n",
      "bert.encoder.layer.4.attention.self.value.weight False\n",
      "bert.encoder.layer.4.attention.self.value.bias False\n",
      "bert.encoder.layer.4.attention.output.dense.weight False\n",
      "bert.encoder.layer.4.attention.output.dense.bias False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.intermediate.dense.weight False\n",
      "bert.encoder.layer.4.intermediate.dense.bias False\n",
      "bert.encoder.layer.4.output.dense.weight False\n",
      "bert.encoder.layer.4.output.dense.bias False\n",
      "bert.encoder.layer.4.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.attention.self.query.weight False\n",
      "bert.encoder.layer.5.attention.self.query.bias False\n",
      "bert.encoder.layer.5.attention.self.key.weight False\n",
      "bert.encoder.layer.5.attention.self.key.bias False\n",
      "bert.encoder.layer.5.attention.self.value.weight False\n",
      "bert.encoder.layer.5.attention.self.value.bias False\n",
      "bert.encoder.layer.5.attention.output.dense.weight False\n",
      "bert.encoder.layer.5.attention.output.dense.bias False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.intermediate.dense.weight False\n",
      "bert.encoder.layer.5.intermediate.dense.bias False\n",
      "bert.encoder.layer.5.output.dense.weight False\n",
      "bert.encoder.layer.5.output.dense.bias False\n",
      "bert.encoder.layer.5.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.attention.self.query.weight False\n",
      "bert.encoder.layer.6.attention.self.query.bias False\n",
      "bert.encoder.layer.6.attention.self.key.weight False\n",
      "bert.encoder.layer.6.attention.self.key.bias False\n",
      "bert.encoder.layer.6.attention.self.value.weight False\n",
      "bert.encoder.layer.6.attention.self.value.bias False\n",
      "bert.encoder.layer.6.attention.output.dense.weight False\n",
      "bert.encoder.layer.6.attention.output.dense.bias False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.intermediate.dense.weight False\n",
      "bert.encoder.layer.6.intermediate.dense.bias False\n",
      "bert.encoder.layer.6.output.dense.weight False\n",
      "bert.encoder.layer.6.output.dense.bias False\n",
      "bert.encoder.layer.6.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.attention.self.query.weight False\n",
      "bert.encoder.layer.7.attention.self.query.bias False\n",
      "bert.encoder.layer.7.attention.self.key.weight False\n",
      "bert.encoder.layer.7.attention.self.key.bias False\n",
      "bert.encoder.layer.7.attention.self.value.weight False\n",
      "bert.encoder.layer.7.attention.self.value.bias False\n",
      "bert.encoder.layer.7.attention.output.dense.weight False\n",
      "bert.encoder.layer.7.attention.output.dense.bias False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.intermediate.dense.weight False\n",
      "bert.encoder.layer.7.intermediate.dense.bias False\n",
      "bert.encoder.layer.7.output.dense.weight False\n",
      "bert.encoder.layer.7.output.dense.bias False\n",
      "bert.encoder.layer.7.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.attention.self.query.weight False\n",
      "bert.encoder.layer.8.attention.self.query.bias False\n",
      "bert.encoder.layer.8.attention.self.key.weight False\n",
      "bert.encoder.layer.8.attention.self.key.bias False\n",
      "bert.encoder.layer.8.attention.self.value.weight False\n",
      "bert.encoder.layer.8.attention.self.value.bias False\n",
      "bert.encoder.layer.8.attention.output.dense.weight False\n",
      "bert.encoder.layer.8.attention.output.dense.bias False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.intermediate.dense.weight False\n",
      "bert.encoder.layer.8.intermediate.dense.bias False\n",
      "bert.encoder.layer.8.output.dense.weight False\n",
      "bert.encoder.layer.8.output.dense.bias False\n",
      "bert.encoder.layer.8.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.attention.self.query.weight False\n",
      "bert.encoder.layer.9.attention.self.query.bias False\n",
      "bert.encoder.layer.9.attention.self.key.weight False\n",
      "bert.encoder.layer.9.attention.self.key.bias False\n",
      "bert.encoder.layer.9.attention.self.value.weight False\n",
      "bert.encoder.layer.9.attention.self.value.bias False\n",
      "bert.encoder.layer.9.attention.output.dense.weight False\n",
      "bert.encoder.layer.9.attention.output.dense.bias False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.intermediate.dense.weight False\n",
      "bert.encoder.layer.9.intermediate.dense.bias False\n",
      "bert.encoder.layer.9.output.dense.weight False\n",
      "bert.encoder.layer.9.output.dense.bias False\n",
      "bert.encoder.layer.9.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.attention.self.query.weight False\n",
      "bert.encoder.layer.10.attention.self.query.bias False\n",
      "bert.encoder.layer.10.attention.self.key.weight False\n",
      "bert.encoder.layer.10.attention.self.key.bias False\n",
      "bert.encoder.layer.10.attention.self.value.weight False\n",
      "bert.encoder.layer.10.attention.self.value.bias False\n",
      "bert.encoder.layer.10.attention.output.dense.weight False\n",
      "bert.encoder.layer.10.attention.output.dense.bias False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.intermediate.dense.weight False\n",
      "bert.encoder.layer.10.intermediate.dense.bias False\n",
      "bert.encoder.layer.10.output.dense.weight False\n",
      "bert.encoder.layer.10.output.dense.bias False\n",
      "bert.encoder.layer.10.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.attention.self.query.weight False\n",
      "bert.encoder.layer.11.attention.self.query.bias False\n",
      "bert.encoder.layer.11.attention.self.key.weight False\n",
      "bert.encoder.layer.11.attention.self.key.bias False\n",
      "bert.encoder.layer.11.attention.self.value.weight False\n",
      "bert.encoder.layer.11.attention.self.value.bias False\n",
      "bert.encoder.layer.11.attention.output.dense.weight False\n",
      "bert.encoder.layer.11.attention.output.dense.bias False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.intermediate.dense.weight False\n",
      "bert.encoder.layer.11.intermediate.dense.bias False\n",
      "bert.encoder.layer.11.output.dense.weight False\n",
      "bert.encoder.layer.11.output.dense.bias False\n",
      "bert.encoder.layer.11.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.output.LayerNorm.bias False\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "# print layers\n",
    "for name, param in hfmodel.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdoutdt.shape=(47, 2)\n",
      "traindt.shape=(100, 3)\n",
      "htmldt.shape=(147, 5)\n",
      "train_df.shape=(75, 3)\n",
      "val_df.shape=(25, 3)\n"
     ]
    }
   ],
   "source": [
    "holdoutdt = htmldt[htmldt[\"label\"] == -1].drop(columns=[\"label\", \"url\", \"label_pred\"])\n",
    "traindt = htmldt[htmldt[\"label\"] > -1].drop(columns=[\"url\", \"label_pred\"])\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    traindt, test_size=0.25, random_state=30, stratify=traindt[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"{holdoutdt.shape=}\")\n",
    "print(f\"{traindt.shape=}\")\n",
    "print(f\"{htmldt.shape=}\")\n",
    "print(f\"{train_df.shape=}\")\n",
    "print(f\"{val_df.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doc_id', 'html', 'label', '__index_level_0__'],\n",
       "    num_rows: 75\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_ds = Dataset.from_pandas(holdoutdt)\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72010be9ac14541a21eda9cc132d689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5956c6361bfd43d98f9e6bdf5cc92950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7f188e65cb4606b69531d0de7d7e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(row):\n",
    "    return tokenizer(row[\"html\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "val_ds = val_ds.map(tokenize_function, batched=True)\n",
    "holdout_ds = holdout_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "train_ds.set_format(type=\"torch\", columns=columns)\n",
    "val_ds.set_format(type=\"torch\", columns=columns)\n",
    "holdout_ds.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics on the validation set\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")  # or macro\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-5,  # you can tune this\n",
    "    output_dir=\"model_output\",  # directory to store model checkpoints\n",
    "    eval_strategy=\"epoch\",  # evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # save model at the end of each epoch\n",
    "    num_train_epochs=10,  # you can tune this\n",
    "    per_device_train_batch_size=8,  # depends on your GPU and data size\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=50,  # logs training progress every 50 steps\n",
    "    load_best_model_at_end=True,  # load the best model after training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hfmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,  # needed for data processing\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c149972dd5c4fcf9798f7bc74d68220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a962048a55d14378a5d72beda160d0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8498169183731079, 'eval_accuracy': 0.6, 'eval_f1': 0.45, 'eval_runtime': 10.6884, 'eval_samples_per_second': 2.339, 'eval_steps_per_second': 0.374, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8928ef6d06421fb8cb4bebb1f478ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8529292941093445, 'eval_accuracy': 0.6, 'eval_f1': 0.45, 'eval_runtime': 11.0768, 'eval_samples_per_second': 2.257, 'eval_steps_per_second': 0.361, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba26aa1dbf9425c9ed07b422128aa70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8623470067977905, 'eval_accuracy': 0.6, 'eval_f1': 0.45, 'eval_runtime': 10.7587, 'eval_samples_per_second': 2.324, 'eval_steps_per_second': 0.372, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70755be2bba841fab2bc6565472d2ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8593291640281677, 'eval_accuracy': 0.6, 'eval_f1': 0.45, 'eval_runtime': 10.7271, 'eval_samples_per_second': 2.331, 'eval_steps_per_second': 0.373, 'epoch': 4.0}\n",
      "{'loss': 0.8927, 'grad_norm': 4.2992401123046875, 'learning_rate': 5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f0fd827d904d52a165e3f8d8d8428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8693068027496338, 'eval_accuracy': 0.6, 'eval_f1': 0.45, 'eval_runtime': 10.3811, 'eval_samples_per_second': 2.408, 'eval_steps_per_second': 0.385, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2cbc7b5cb44a91b0c89a55fa0a026b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8807011246681213, 'eval_accuracy': 0.6, 'eval_f1': 0.45, 'eval_runtime': 10.3934, 'eval_samples_per_second': 2.405, 'eval_steps_per_second': 0.385, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e2b2748396416e9368528a13ba1086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8860752582550049, 'eval_accuracy': 0.64, 'eval_f1': 0.5326495726495726, 'eval_runtime': 11.1775, 'eval_samples_per_second': 2.237, 'eval_steps_per_second': 0.358, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a86a62eb8c414390b011765d1dbf52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8930975198745728, 'eval_accuracy': 0.6, 'eval_f1': 0.5061052631578947, 'eval_runtime': 10.4553, 'eval_samples_per_second': 2.391, 'eval_steps_per_second': 0.383, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac518ac14c6948b3825a429d2d243177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.894818902015686, 'eval_accuracy': 0.64, 'eval_f1': 0.5326495726495726, 'eval_runtime': 10.3032, 'eval_samples_per_second': 2.426, 'eval_steps_per_second': 0.388, 'epoch': 9.0}\n",
      "{'loss': 0.766, 'grad_norm': 6.987075328826904, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11891888ada64afbbc9ba566070b75c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8941504955291748, 'eval_accuracy': 0.64, 'eval_f1': 0.5326495726495726, 'eval_runtime': 11.4463, 'eval_samples_per_second': 2.184, 'eval_steps_per_second': 0.349, 'epoch': 10.0}\n",
      "{'train_runtime': 540.6263, 'train_samples_per_second': 1.387, 'train_steps_per_second': 0.185, 'train_loss': 0.8293684005737305, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f016553e86e43a9a07cfdc89461991d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation results: {'eval_loss': 0.8498169183731079, 'eval_accuracy': 0.6, 'eval_f1': 0.45, 'eval_runtime': 10.3811, 'eval_samples_per_second': 2.408, 'eval_steps_per_second': 0.385, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferece on holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011b99299e764f18bdd21969cbd3ea8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "holdout_preds = trainer.predict(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   1\n",
       "row_0    \n",
       "0       8\n",
       "1      15\n",
       "2       2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.argmax(holdout_preds.predictions, axis=-1)\n",
    "pd.crosstab(val_ds[\"label\"], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = (\n",
    "    np.argmax(holdout_preds.predictions, axis=-1) + 1\n",
    ")  # convert back to 1-index\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer model did not perform well on the validation set. This is because token size allowed on the bert model is limited to 512. The html pages have 200 - 36K words in total. The model cannot fully capture the entire contents for many observation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) TF-IDF + SVM / RF / XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Collect lemmas, excluding stop words and punctuation\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        # Exclude stop words, punctuation, and pronouns like '-PRON-'\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "\n",
    "    # Join the lemmas back into a string\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "\n",
    "htmldt[\"html_clean\"] = htmldt[\"html\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      147.000000\n",
      "mean      1883.755102\n",
      "std       3867.911788\n",
      "min        206.000000\n",
      "50%        966.000000\n",
      "95%       4655.500000\n",
      "max      36214.000000\n",
      "Name: html, dtype: float64\n",
      "count      147.000000\n",
      "mean      1223.755102\n",
      "std       2097.218388\n",
      "min        161.000000\n",
      "50%        666.000000\n",
      "75%       1332.000000\n",
      "95%       2955.000000\n",
      "max      19069.000000\n",
      "Name: html_clean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(htmldt.html.str.split().apply(len).describe(percentiles=[0.95]))\n",
    "print(htmldt.html_clean.str.split().apply(len).describe(percentiles=[0.75, 0.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1500)\n",
    "Xarray = tfidf.fit_transform(htmldt[\"html_clean\"]).toarray()\n",
    "Xdf = pd.DataFrame(Xarray, columns=tfidf.get_feature_names_out())\n",
    "dt_tfidf = pd.concat([htmldt, Xdf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dt_tfidf[dt_tfidf[\"label\"] > -1].drop(\n",
    "    columns=[\"doc_id\", \"url\", \"label_pred\", \"html\", \"html_clean\", \"label\"]\n",
    ")\n",
    "y = dt_tfidf[dt_tfidf[\"label\"] > -1][\"label\"]\n",
    "holdout_X = dt_tfidf[dt_tfidf[\"label\"] == -1]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=30, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6333333333333333\n",
      "F1: 0.5533333333333333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0   1\n",
       "label       \n",
       "0      2   7\n",
       "1      1  17\n",
       "2      0   3"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(kernel=\"sigmoid\", gamma=1.0)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_val)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred)}\")\n",
    "print(f\"F1: {f1_score(y_val, y_pred, average='weighted')}\")\n",
    "pd.crosstab(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "F1: 0.5786561264822134\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0   1\n",
       "label       \n",
       "0      2   7\n",
       "1      0  18\n",
       "2      0   3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=700, max_depth=5, max_samples=0.9, random_state=30\n",
    ")\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_val)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred)}\")\n",
    "print(f\"F1: {f1_score(y_val, y_pred, average='weighted')}\")\n",
    "pd.crosstab(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7666666666666667\n",
      "F1: 0.7369409660107334\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0   1  2\n",
       "label          \n",
       "0      4   5  0\n",
       "1      0  18  0\n",
       "2      0   2  1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = XGBClassifier(n_estimators=600, max_depth=4, random_state=30)\n",
    "\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_pred = xgbc.predict(X_val)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred)}\")\n",
    "print(f\"F1: {f1_score(y_val, y_pred, average='weighted')}\")\n",
    "pd.crosstab(y_val, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
